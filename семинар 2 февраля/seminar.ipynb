{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Быстрее ли numpy?\n",
    "\n",
    "Говорилось, что вычисления с использованием библиотеки numpy выполняются с большей скоростью. Для проверки посчитаем скалярное произведение двух векторов с использованием numpy и без."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нам понадобится библиотека time для временных замеров:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A = np.random.normal(size = (100000,))\n",
    "B = np.random.normal(size = (100000,))\n",
    "A_list, B_list = list(A), list(B)\n",
    "\n",
    "type(A), type(A_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем скорость работы в одном случае:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start = time.clock()\n",
    "res = 0.\n",
    "###################################################\n",
    "# Место для скалярного умножения A_list и B_list, #\n",
    "#                   вычисленного с помощью цикла. #\n",
    "###################################################\n",
    "print(('Программе потребовалось %.4f секунды '\n",
    "       'для вычисления скалярного умножения '\n",
    "       'стандартными средствами python') % (time.clock() - start)) # время выполнения в секундах"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И во втором:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start = time.clock()\n",
    "###################################################\n",
    "# Место для скалярного умножения A_list и B_list, #\n",
    "#            вычисленного с помощью методов numpy #\n",
    "###################################################\n",
    "print(('Программе потребовалось %.4f секунды '\n",
    "       'для вычисления скалярного умножения '\n",
    "       'с помощью методов numpy') % (time.clock() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Линейная регрессия: подсчет функционалов качества"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим модельную задачу линейной регрессии: имеется выборка из $N = 100$ объектов, каждый из них обладает $d = 2$ действительным признаками и значением наблюдаемой величины. Признаки объектов образуют матрицу $X$ (размера $N \\times d$, $N$ строчек и $d$ столбцов), наблюдаемые величины матрицу $Y$\n",
    "\n",
    "Решая задачу линейной регрессии, мы будем искать зависимость между $X$ и $Y$ в форме $y_i = f(x_i) = a x_{i1} + b x_{i2} + c$. \n",
    "\n",
    "Ниже случайным образом сгенерированы данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = 100\n",
    "N_test = 20\n",
    "d = 2\n",
    "# x_1 умножаем на 3, x_2 умножаем на 5, добавляем 8, а потом добавляем шум\n",
    "X = 1 + 2 * np.random.randn(N, d)\n",
    "Y = np.sum(X * np.array([3, 5]), axis = 1) + 8 + 1.2 * np.random.randn(N)\n",
    "\n",
    "X_test = 1 + 2 * np.random.randn(N_test, d)\n",
    "Y_test = np.sum(X_test * np.array([3, 5]), axis = 1) + 8 + 1.2 * np.random.randn(N_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сейчас мы для решения задачи воспользуемся стандартной функцией библиотеки sklearn. Она выдает лишь два значения, коэффициенты $a$ и $b$. Коэффициент $c$ можно по ним и по данным."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(X, Y)\n",
    "# a, b:\n",
    "a, b = regr.coef_\n",
    "# c:\n",
    "c = np.mean(Y - np.dot(X, regr.coef_))\n",
    "(a, b, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изображаем её:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "range_x = np.linspace(np.min(X[:, 0]), np.max(X[:, 0]), 10)\n",
    "range_y = np.linspace(np.min(X[:, 1]), np.max(X[:, 1]), 10)\n",
    "xx, yy = np.meshgrid(range_x, range_y)\n",
    "\n",
    "ax.view_init(elev=0., azim=15)\n",
    "ax.plot_surface(xx, yy, a * xx + b * yy + c, alpha = 0.5)\n",
    "ax.scatter(X[:, 0], X[:, 1], c = 'b')\n",
    "ax.scatter(X[:, 0], X[:, 1], Y, c = 'r');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У нас есть значений $Y$, посчитаем предсказания модели f(X): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При сравнении предсказаний и истинных значений на помощь нам может придти линейная алгебра. Оба этих массива представляют точки в линейном пространстве, посчитаем расстояние в между ними!\n",
    "\n",
    "Вообще говоря, расстояния (метрики) в математике огромным количеством способов, каждый из этих способов может задавать нам критерий качества работы модели. Приведем несколько примеров:\n",
    "\n",
    "1. Евклидова метрика, квадратичная\n",
    "\n",
    "    $ \\rho_2(x, y) = \\sqrt{\\sum_{i = 1}^{N} (x_i - y_i)^2} $\n",
    "\n",
    "2. $L_1$, манхэттенская\n",
    "\n",
    "    $ \\rho_1(x, y) = \\sum_{i = 1}^N |x_i - y_i| $\n",
    "\n",
    "3. $L_\\infty$\n",
    "\n",
    "    $ \\rho_{\\infty} = \\max_i |x_i - y_i|$\n",
    "\n",
    "Все они объединяются одной формулой:\n",
    "$\\rho_p(x, y) = (\\sum_{i=1}^n |x_i - y_i|^p)^{1/p}$\n",
    "\n",
    "Вычислить значение функционала качества, определяемого расстоянием $\\rho_2$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверка:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.linalg.norm(loss, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычислить значение функционала качества, определяемого расстоянием $\\rho_1$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Проверка:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.linalg.norm(loss, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычислить значение функционала качества, определяемого расстоянием $\\rho_{\\infty}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Провекра:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.linalg.norm(loss, np.inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Преобразования данных\n",
    "\n",
    "Алгоритмы машинного обучаения бывают чувствительны к значениям входных данных, чтобы избежать возможных проблем данные нормализуют.\n",
    "\n",
    "Первым делом преобразуем X так, чтобы среднее значение каждого признака было равно нулю:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь изменим масштаб так, чтобы каждый признак лежал в отрезке $[-1, 1]$. Домножим каждый признак для этого на подходящее число:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Линейная регрессия своими руками\n",
    "\n",
    "Коэффициенты линейной регрессии выражаются по формуле $(X^T X)^{-1}  X^T y$. Вычислить её своими руками почти также просто как воспользоваться библиотечной функцией, заодно мы поупражняемся в матричных операциях.\n",
    "\n",
    "Вычислять выражение будем слева направо. Вычислять произведения матриц можно несколькими способами, воспользуемся методов *numpy.dot* для вычисления $X^T X$. В numpy X.T возвращает транспонированную матрицу X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "step_1 = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратную матрицу можно посчитать с помощью библиотечной функции:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "step_2 = np.linalg.inv(step_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Второе произведение посчитаем также:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "step_3 = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А домножение на произведем с помощью покомпонентного умножения и метода np.sum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "step_4 = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Посчитаем производную\n",
    "\n",
    "Не всегда удается выписать явную формулу, по которой можно вычислить параметры. Да и явные формулы не всегда уместны. В таких случаях можно искать минимальные значения функционала потерь, обсудим как можно искать минимумы с помощью компьютера.\n",
    "\n",
    "## 5.1 Пример с многочленами\n",
    "\n",
    "Воспользуемся библиотечной функцией, которая считает значения многочлена. На вход она принимает массив аргументов и массив коэффициентов многочлена (от младшего к старшему)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from numpy.polynomial.polynomial import polyval\n",
    "\n",
    "f = np.array([-2, -8, -5, 10, 1, -2])\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "xx = np.linspace(-2.1, 2.1, 500)\n",
    "ax.set_xlim(-2.1, 2.1)\n",
    "ax.plot(xx, polyval(xx, f));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишите значение аналогичной функции, вычисляющей значения производной многочлена:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def derivative(x, f):\n",
    "    #########################\n",
    "    # Место для производной #\n",
    "    #########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Производная не только дает необходимые условия экстремума, но и позволяет приближать функцию! Положим $x_0 + \\Delta x = x$\n",
    "\n",
    "$f(x_0 + \\Delta x) \\approx f(x_0) + f'(x_0) \\Delta x$\n",
    "\n",
    "Если взять достаточно малый $\\Delta x$ того же знака, что и $-f'(x_0$), то \n",
    "$f'(x_0) \\Delta x < 0$, а из первой формулы следует $f(x_0 + \\Delta x) < f(x_0) + f'(x_0) \\Delta x$.\n",
    "\n",
    "Поэтому можно ожидать, что в $f(x_0 + \\Delta x) $ у функции будет меньшее значение!\n",
    "\n",
    "На этом наблюдении основан метод градиентного спуска. Для некоторого положительного $\\nu$ на каждом шаге полагаем $\\Delta x = -\\nu f'(x_{old})$ и переходим из старой точки $x_{old}$ в новую точку $x_{new} = x_{old} - \\nu  f'(x_{old}) $\n",
    "\n",
    "Спуск:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "T = 20\n",
    "x = np.zeros(T)\n",
    "x[0] = 0\n",
    "nu = 0.01\n",
    "for t in np.arange(1, T, 1):\n",
    "    ###########################\n",
    "    # Шаг градиентного спуска #\n",
    "    ###########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Иллюстрация работы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "xx = np.linspace(-2.1, 2.1, 500)\n",
    "ax.set_xlim(-2.1, 2.1)\n",
    "ax.plot(xx, np.polynomial.polynomial.polyval(xx, f), color = (1, 0.5, 0.5))\n",
    "#for i in len(tt)\n",
    "ax.plot(x, np.polynomial.polynomial.polyval(x, f), 'o', color = (0.5, 0.5, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2* Подбор параметром линейной регрессии\n",
    "\n",
    "С помощью градиентного спуска можно также найти параметры линейной регрессии! Достаточно правильно вычислить производную квадратичной ошибки и найти её минимум относительно параметров. (В случае одномерного градиентного спуска мы брали $\\Delta x$ того же знака, что и $f'$, теперь мы будем брать $\\Delta x$ того же направления, что и $grad f$ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
